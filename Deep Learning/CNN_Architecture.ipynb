{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        "Filters (Kernels)\n",
        "\n",
        "* Filters are small matrices (e.g., 3×3, 5×5) used to scan over the input image.\n",
        "\n",
        "* Their role is to extract features such as edges, textures, curves, colors, patterns, etc.\n",
        "\n",
        "* Each filter detects a specific type of feature.\n",
        "\n",
        "* During training, CNN learns filter values automatically so that the network can recognize patterns important for classification.\n",
        "\n",
        "Feature Maps\n",
        "\n",
        "* A feature map is the output produced after applying a filter over the input.\n",
        "\n",
        "* It represents where a particular feature is found in the image.\n",
        "\n",
        "* Feature maps highlight important patterns while reducing irrelevant information.\n",
        "\n",
        "* Stacking multiple feature maps helps the CNN learn low-level features (edges), mid-level features (shapes), and high-level features (objects)."
      ],
      "metadata": {
        "id": "2SjFq5LFLtqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "**1. Padding**\n",
        "\n",
        "Padding refers to adding extra rows/columns of zeros around the input image.\n",
        "\n",
        "Why padding is used?\n",
        "\n",
        "* To preserve spatial dimensions after convolution.\n",
        "\n",
        "* To avoid losing important information near edges.\n",
        "\n",
        "* To allow deeper networks (more layers) without shrinking the image too fast.\n",
        "\n",
        "Types\n",
        "\n",
        "* Valid padding (No padding)\n",
        "Output becomes smaller than input.\n",
        "\n",
        "* Same padding (Zero padding added)\n",
        "Output size is same as input.\n",
        "\n",
        "Output size=((N−F+2P​)/S) +1\n",
        "\n",
        "**2. Stride**\n",
        "\n",
        "Stride refers to how many pixels the filter moves each time during convolution.\n",
        "\n",
        "Effect of Stride\n",
        "\n",
        "* Stride = 1:\n",
        "Filter moves one pixel → larger output\n",
        "\n",
        "* Stride = 2 or more:\n",
        "Filter jumps more pixels → smaller output, downsampling occurs"
      ],
      "metadata": {
        "id": "hGYyhOlJL5VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?\n",
        "\n",
        "The receptive field of a neuron in a CNN is the region of the input image that influences (or contributes to) the value of that neuron.\n",
        "\n",
        "Why is it important in deep CNN architectures?\n",
        "\n",
        "1. Higher layers see larger portions of the image\n",
        "\n",
        "* In early layers, neurons have small receptive fields (e.g., edges, corners).\n",
        "\n",
        "* In deeper layers, receptive fields grow larger, allowing the network to understand bigger patterns like shapes, faces, or objects.\n",
        "\n",
        "2. Helps capture complex, global information\n",
        "\n",
        "Deep networks combine information from many small regions to understand:\n",
        "\n",
        "* High-level features\n",
        "\n",
        "* Object parts\n",
        "\n",
        "* Entire objects in the image\n",
        "\n",
        "3. Important for tasks like detection and segmentation\n",
        "\n",
        "Large receptive fields allow deep CNNs to:\n",
        "\n",
        "* Recognize objects regardless of size\n",
        "\n",
        "* Understand spatial relationships\n",
        "\n",
        "* Capture context (background, surroundings)\n",
        "\n",
        "4. Avoids losing information\n",
        "\n",
        "* If the receptive field is too small, deeper layers will not “see” enough of the input, leading to poor understanding."
      ],
      "metadata": {
        "id": "WUfaOfQxMUSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.\n",
        "\n",
        "1. Effect of Filter Size on Number of Parameters\n",
        "\n",
        "The filter size directly affects the total number of trainable parameters in a CNN.\n",
        "\n",
        "Formula for parameters in one filter\n",
        "\n",
        "Parameters per filter=(F×F×Cin​)+1 (bias)\n",
        "\n",
        "How filter size affects parameters\n",
        "\n",
        "* Larger filters (e.g., 7×7) → more parameters\n",
        "\n",
        "* Smaller filters (e.g., 3×3) → fewer parameters\n",
        "\n",
        "2. Effect of Stride on Number of Parameters\n",
        "\n",
        "Stride does NOT change the number of trainable parameters\n",
        "because:\n",
        "\n",
        "* Stride only affects how the filter moves\n",
        "\n",
        "* It does not change filter size or number of filters\n",
        "\n",
        "What stride affects:\n",
        "\n",
        "* Output feature map size\n",
        "\n",
        "* Amount of computation (FLOPs)\n",
        "\n",
        "* Downsampling level\n",
        "\n",
        "But NOT:\n",
        "\n",
        "* Filter weights\n",
        "\n",
        "* Total trainable parameters"
      ],
      "metadata": {
        "id": "fqo2ZOyJNEuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "1. LeNet-5 (1998)\n",
        "\n",
        "Depth\n",
        "\n",
        "* Very shallow\n",
        "\n",
        "* About 7 layers (including conv + pooling + fully connected)\n",
        "\n",
        "Filter Sizes\n",
        "\n",
        "* 5×5 convolution filters\n",
        "\n",
        "* Simple architecture, low computational load\n",
        "\n",
        "Performance\n",
        "\n",
        "* Designed for digit recognition (MNIST)\n",
        "\n",
        "* Performs well on small grayscale images\n",
        "\n",
        "* Not suitable for large-scale datasets like ImageNet\n",
        "\n",
        "* Very lightweight and fast\n",
        "\n",
        "2. AlexNet (2012)\n",
        "\n",
        "Depth\n",
        "\n",
        "* Much deeper than LeNet\n",
        "\n",
        "* 8 layers (5 conv + 3 fully connected)\n",
        "\n",
        "* Introduced ReLU, dropout, and data augmentation\n",
        "\n",
        "Filter Sizes\n",
        "\n",
        "* First layer: 11×11 filters (large)\n",
        "\n",
        "* Later layers: 5×5 and 3×3 filters\n",
        "\n",
        "* Used overlapping max pooling\n",
        "\n",
        "Performance\n",
        "\n",
        "* Won ImageNet 2012 by a large margin\n",
        "\n",
        "* First successful large-scale CNN\n",
        "\n",
        "* Handled RGB images (224×224)\n",
        "\n",
        "* High performance but computationally heavy\n",
        "\n",
        "3. VGG (2014) – VGG-16 / VGG-19\n",
        "\n",
        "Depth\n",
        "\n",
        "* Much deeper network\n",
        "\n",
        "* 16–19 layers\n",
        "\n",
        "* Very uniform design\n",
        "\n",
        "Filter Sizes\n",
        "\n",
        "* Only 3×3 convolution filters\n",
        "\n",
        "* Stacked multiple small filters instead of large filters\n",
        "\n",
        "* More parameters but better feature extraction\n",
        "\n",
        "Performance\n",
        "\n",
        "* Very high accuracy on ImageNet\n",
        "\n",
        "* Better performance than AlexNet due to depth + small filters\n",
        "\n",
        "* Computationally expensive → more memory usage\n",
        "\n",
        "* Still widely used for transfer learning"
      ],
      "metadata": {
        "id": "-obvZ4sbNiiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GVXwkwuYOFL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Load and preprocess data\n",
        "# ---------------------------\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape to (samples, height, width, channels)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test  = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Normalize pixel values (0-255 → 0-1)\n",
        "x_train = x_train / 255.0\n",
        "x_test  = x_test / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test  = to_categorical(y_test, 10)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Build CNN Model\n",
        "# ---------------------------\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Compile the model\n",
        "# ---------------------------\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Train the model\n",
        "# ---------------------------\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Evaluate the model\n",
        "# ---------------------------\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_acc)\n",
        "print(\"Test Loss:\", test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "v2Ptc6QvON_M",
        "outputId": "d71817d3-b65d-4f15-aabc-1f1f5aafd73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_19 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_20 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - accuracy: 0.8883 - loss: 0.3771 - val_accuracy: 0.9860 - val_loss: 0.0469\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0514 - val_accuracy: 0.9880 - val_loss: 0.0406\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0344 - val_accuracy: 0.9908 - val_loss: 0.0349\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9921 - loss: 0.0246 - val_accuracy: 0.9897 - val_loss: 0.0330\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0166 - val_accuracy: 0.9898 - val_loss: 0.0386\n",
            "313/313 - 1s - 4ms/step - accuracy: 0.9872 - loss: 0.0369\n",
            "\n",
            "Test Accuracy: 0.9872000217437744\n",
            "Test Loss: 0.036863744258880615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture."
      ],
      "metadata": {
        "id": "LnO5RvW-ORI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 1. Import Libraries\n",
        "# ---------------------------\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Load CIFAR-10 dataset\n",
        "# ---------------------------\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(\"Training data shape:\", x_train.shape)\n",
        "print(\"Test data shape:\", x_test.shape)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Preprocessing\n",
        "# ---------------------------\n",
        "\n",
        "# Normalize pixel values (0–255 → 0–1)\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# One-hot encode labels (10 classes)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Create CNN Model\n",
        "# ---------------------------\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional block\n",
        "model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)))\n",
        "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Second convolutional block\n",
        "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flatten + Dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Compile the Model\n",
        "# ---------------------------\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Train the Model\n",
        "# ---------------------------\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1)\n",
        "\n",
        "# ---------------------------\n",
        "# 7. Evaluate the Model\n",
        "# ---------------------------\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"\\nTest Accuracy:\", test_acc)\n",
        "print(\"Test Loss:\", test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "zCOP8dCzOSbM",
        "outputId": "49fdde6f-d95c-4256-ebec-a0a4fe9035ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (50000, 32, 32, 3)\n",
            "Test data shape: (10000, 32, 32, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_21 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_22 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m2,097,664\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,168,362\u001b[0m (8.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,168,362</span> (8.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,168,362\u001b[0m (8.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,168,362</span> (8.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - accuracy: 0.3135 - loss: 1.8415 - val_accuracy: 0.5556 - val_loss: 1.2444\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5667 - loss: 1.2123 - val_accuracy: 0.6806 - val_loss: 0.9379\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6531 - loss: 0.9839 - val_accuracy: 0.7160 - val_loss: 0.8301\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.6937 - loss: 0.8744 - val_accuracy: 0.7220 - val_loss: 0.7846\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7239 - loss: 0.7812 - val_accuracy: 0.7492 - val_loss: 0.7246\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7441 - loss: 0.7231 - val_accuracy: 0.7602 - val_loss: 0.7023\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7654 - loss: 0.6623 - val_accuracy: 0.7668 - val_loss: 0.6751\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7832 - loss: 0.6185 - val_accuracy: 0.7534 - val_loss: 0.7066\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7920 - loss: 0.5807 - val_accuracy: 0.7718 - val_loss: 0.6734\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8031 - loss: 0.5558 - val_accuracy: 0.7892 - val_loss: 0.6268\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7724 - loss: 0.6748\n",
            "\n",
            "Test Accuracy: 0.7739999890327454\n",
            "Test Loss: 0.6731377243995667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation"
      ],
      "metadata": {
        "id": "wqJ0bcrOOj-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 1. Import Libraries\n",
        "# ---------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2. Device Configuration\n",
        "# ---------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 3. MNIST Dataset + Transformations\n",
        "# ---------------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True,\n",
        "                               transform=transform, download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False,\n",
        "                              transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 4. Define CNN Model (Auto-calculates flatten size)\n",
        "# ---------------------------------------------\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Placeholder, will be updated once model sees data\n",
        "        self.flatten_dim = None\n",
        "        self.fc1 = None\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def compute_flatten_dim(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x.size(1)\n",
        "\n",
        "    def build_fc(self, flatten_dim):\n",
        "        self.flatten_dim = flatten_dim\n",
        "        self.fc1 = nn.Linear(flatten_dim, 128).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Build FC layer dynamically on first run\n",
        "        if self.fc1 is None:\n",
        "            flatten_dim = self.compute_flatten_dim(x)\n",
        "            self.build_fc(flatten_dim)\n",
        "\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "print(model)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 5. Loss and Optimizer\n",
        "# ---------------------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 6. Training Loop\n",
        "# ---------------------------------------------\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 7. Evaluation\n",
        "# ---------------------------------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(\"\\nTest Accuracy:\", accuracy, \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWiVhMSJOprH",
        "outputId": "2db6245f-9045-4db2-8ae8-43f77b44680b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "Epoch [1/5] Loss: 0.4510\n",
            "Epoch [2/5] Loss: 0.1528\n",
            "Epoch [3/5] Loss: 0.1079\n",
            "Epoch [4/5] Loss: 0.0880\n",
            "Epoch [5/5] Loss: 0.0761\n",
            "\n",
            "Test Accuracy: 97.61 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model.\n"
      ],
      "metadata": {
        "id": "kdt1H2YTOso3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ----------------------------------\n",
        "# 1. ImageDataGenerator for Preprocessing\n",
        "# ----------------------------------\n",
        "\n",
        "train_path = \"dataset/train\"\n",
        "val_path = \"dataset/validation\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,         # normalize images\n",
        "    rotation_range=20,       # data augmentation\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# Create Generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(150, 150),   # resize all images\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # for multi-class classification\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_path,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# ----------------------------------\n",
        "# 2. Build CNN Model\n",
        "# ----------------------------------\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ----------------------------------\n",
        "# 3. Compile Model\n",
        "# ----------------------------------\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ----------------------------------\n",
        "# 4. Train the Model\n",
        "# ----------------------------------\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# ----------------------------------\n",
        "# 5. Evaluate the Model\n",
        "# ----------------------------------\n",
        "\n",
        "loss, acc = model.evaluate(val_generator)\n",
        "print(\"Validation Accuracy:\", acc)\n",
        "print(\"Validation Loss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "ww8UokW6OwjQ",
        "outputId": "d76b5d6b-a03e-4507-ef80-0a82f44083ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 images belonging to 2 classes.\n",
            "Found 6 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_23 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_24 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_25 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m9,470,208\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m514\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,470,208</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,563,970\u001b[0m (36.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,563,970</span> (36.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,563,970\u001b[0m (36.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,563,970</span> (36.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.3333 - loss: 0.7070 - val_accuracy: 1.0000 - val_loss: 0.0585\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.1855 - val_accuracy: 1.0000 - val_loss: 5.5967e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 5.5042e-04 - val_accuracy: 1.0000 - val_loss: 5.9605e-08\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 7.9473e-07 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Validation Accuracy: 1.0\n",
            "Validation Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit.\n"
      ],
      "metadata": {
        "id": "mP93qs9iO1J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Problem framing & constraints\n",
        "\n",
        "* Binary classification: Normal vs Pneumonia.\n",
        "\n",
        "* Confirm available data (CXR views, image sizes, DICOM vs PNG/JPEG), dataset size, labels quality (radiologist-reviewed?), and privacy/consent.\n",
        "\n",
        "2. Data preparation\n",
        "\n",
        "* Collect DICOM/PNG, deidentify (remove PHI), store securely.\n",
        "\n",
        "* Split patient-wise into Train / Val / Test (e.g., 70/15/15) to avoid patient leakage.\n",
        "\n",
        "* Handle class imbalance (class weights, oversampling, focal loss).\n",
        "\n",
        "3. Preprocessing & augmentation\n",
        "\n",
        "* Read, convert to float32, normalize (0–1 or mean/std), resize to model input (e.g., 224×224).\n",
        "\n",
        "* Augmentation: rotations, translations, horizontal flips (careful: some clinical tasks shouldn't flip), brightness/contrast, random crops — keep clinically plausible transforms.\n",
        "\n",
        "4. Model selection\n",
        "\n",
        "* Prefer transfer learning (pretrained backbones: EfficientNetB0/B3, ResNet50, DenseNet121) — DenseNet/ResNet/EfficientNet commonly used for CXR.\n",
        "\n",
        "* Replace top with global pooling + dense + dropout → 1 output (sigmoid) for binary.\n",
        "\n",
        "5. Training\n",
        "\n",
        "* Loss: binary_crossentropy (or focal loss).\n",
        "\n",
        "* Optimizer: AdamW/Adam, learning-rate schedule (Cosine/ReduceLROnPlateau).\n",
        "\n",
        "* Use early stopping on validation AUC.\n",
        "\n",
        "* Track metrics: AUC-ROC, sensitivity (recall), specificity, F1, confusion matrix.\n",
        "\n",
        "6. Explainability & uncertainty\n",
        "\n",
        "* Generate Grad-CAM / saliency maps for each prediction.\n",
        "\n",
        "* Use Bayesian techniques or MC dropout for uncertainty estimation if needed.\n",
        "\n",
        "7. Evaluation & clinical validation\n",
        "\n",
        "* Evaluate on held-out test set by patient.\n",
        "\n",
        "* Compute ROC, PR curve, sensitivity @ fixed specificity, per-subgroup performance (age, device).\n",
        "\n",
        "* Clinical validation: retrospective study, then prospective, then reader study vs radiologists.\n",
        "\n",
        "8. Model packaging & serving\n",
        "\n",
        "* Save model (SavedModel / .h5 / TorchScript).\n",
        "\n",
        "* Serve via lightweight API (FastAPI / Flask) or embed directly in Streamlit for small models.\n",
        "\n",
        "* Containerize with Docker.\n",
        "\n",
        "9. Web app UX\n",
        "\n",
        "* Upload X-ray (DICOM/PNG/JPEG), show predictions + probabilities + Grad-CAM overlay, show confidence, warning that it’s investigational.\n",
        "\n",
        "10. Deployment & MLOps\n",
        "\n",
        "* CI for tests, model versioning (MLflow/DVC), monitoring (data drift, performance), logging (no PHI), A/B testing, rollback.\n",
        "\n",
        "11. Security, privacy & compliance\n",
        "\n",
        "* Encryption in transit & at rest, access control, audit logs, HIPAA/DPDP compliance as required.\n",
        "\n",
        "12. Post-deployment\n",
        "\n",
        "* Model monitoring, periodic re-evaluation, clinician feedback loop."
      ],
      "metadata": {
        "id": "xRzdy7NZPAIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "def load_and_preprocess(path):\n",
        "    # path: file path to image (png/jpg) or pixel array\n",
        "    img = tf.io.read_file(path)\n",
        "    try:\n",
        "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    except:\n",
        "        raise\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)  # 0-1\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    return img  # shape (IMG_SIZE, IMG_SIZE, 3) float32 0-1\n",
        "\n",
        "# Example augmentation pipeline using tf.keras layers (applied in training only)\n",
        "data_augment = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.05),\n",
        "    tf.keras.layers.RandomZoom(0.05),\n",
        "    tf.keras.layers.RandomTranslation(0.02, 0.02),\n",
        "])\n"
      ],
      "metadata": {
        "id": "sMjetGHnQ2NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_build.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_model(img_size=224, base_model_name='EfficientNetB0', dropout=0.5):\n",
        "    base = tf.keras.applications.EfficientNetB0(\n",
        "        include_top=False, input_shape=(img_size, img_size, 3), weights='imagenet'\n",
        "    )\n",
        "    base.trainable = False  # freeze initially\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(img_size, img_size, 3))\n",
        "    x = inputs\n",
        "    x = tf.keras.applications.efficientnet.preprocess_input(x)\n",
        "    x = base(x, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "S_qYigaNQ80d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py implementation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks, optimizers, losses, metrics\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define IMG_SIZE to match build_model expectations (from model_build.py)\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values (0-255 -> 0-1) - keep original size for now\n",
        "x_train_norm = x_train_raw.astype('float32') / 255.0\n",
        "x_test_norm = x_test_raw.astype('float32') / 255.0\n",
        "\n",
        "# Create dummy binary labels for demonstration (e.g., classifying if an image is 'airplane' or 'not airplane')\n",
        "# The EfficientNet model expects 1 output for binary classification with sigmoid activation.\n",
        "# Here, we'll label class 0 (airplane) as positive (1) and all others as negative (0).\n",
        "y_train_binary = (y_train_raw == 0).astype(float)\n",
        "y_test_binary = (y_test_raw == 0).astype(float)\n",
        "\n",
        "# Use a small subset of the *original sized* data for faster demonstration\n",
        "# This splits the data before memory-intensive resizing\n",
        "x_train_subset, _, y_train_subset, _ = train_test_split(x_train_norm, y_train_binary, test_size=0.9, random_state=42) # Use 10% of training data\n",
        "x_val_subset, _, y_val_subset, _ = train_test_split(x_test_norm, y_test_binary, test_size=0.9, random_state=42)     # Use 10% of test data for validation\n",
        "\n",
        "# Function to resize images within the tf.data pipeline\n",
        "def resize_image(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    return image, label\n",
        "\n",
        "# Create tf.data.Dataset objects and apply resizing with .map()\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_subset, y_train_subset)) \\\n",
        "    .map(resize_image, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "    .batch(BATCH_SIZE) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val_subset, y_val_subset)) \\\n",
        "    .map(resize_image, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "    .batch(BATCH_SIZE) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# Build, compile, and train the model\n",
        "# 'build_model' is expected to be defined in a preceding cell (S_qYigaNQ80d)\n",
        "model = build_model(img_size=IMG_SIZE)\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=losses.BinaryCrossentropy(),\n",
        "    metrics=[metrics.AUC(name='auc'), metrics.Recall(name='sensitivity')]\n",
        ")\n",
        "\n",
        "# Example callbacks\n",
        "cbs = [\n",
        "    callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=3, mode='max'),\n",
        "    callbacks.EarlyStopping(monitor='val_auc', patience=6, mode='max', restore_best_weights=True),\n",
        "]\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=val_dataset,\n",
        "                    epochs=5, # Reduced epochs for demonstration\n",
        "                    callbacks=cbs)\n",
        "print(\"Model training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMc3WPPKQ_TW",
        "outputId": "dcf8eaad-637b-4d17-af1b-a46064d642b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 187ms/step - auc: 0.4888 - loss: 0.4062 - sensitivity: 0.0759 - val_auc: 0.5000 - val_loss: 0.3346 - val_sensitivity: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 2/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - auc: 0.4983 - loss: 0.3375 - sensitivity: 0.0000e+00 - val_auc: 0.5000 - val_loss: 0.3345 - val_sensitivity: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 3/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - auc: 0.4658 - loss: 0.3407 - sensitivity: 0.0000e+00 - val_auc: 0.5000 - val_loss: 0.3361 - val_sensitivity: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - auc: 0.5011 - loss: 0.3343 - sensitivity: 0.0000e+00 - val_auc: 0.5000 - val_loss: 0.3355 - val_sensitivity: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - auc: 0.5248 - loss: 0.3319 - sensitivity: 0.0000e+00 - val_auc: 0.5000 - val_loss: 0.3348 - val_sensitivity: 0.0000e+00 - learning_rate: 5.0000e-05\n",
            "Model training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradcam.py (simple version)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        loss = predictions[:, 0]  # sigmoid output\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + 1e-8)\n",
        "    heatmap = tf.image.resize(heatmap[..., tf.newaxis], (img_array.shape[1], img_array.shape[2])).numpy()\n",
        "    return heatmap.squeeze()\n"
      ],
      "metadata": {
        "id": "34MQawLRRWB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new_cell_1_for_streamlit_install"
      },
      "source": [
        "# Install Streamlit\n",
        "!pip install streamlit -qq\n",
        "# If you encounter a ModuleNotFoundError even after running this, you might need to restart your Colab runtime (Runtime > Restart runtime) and then run all cells."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "# --- Placeholder for load_and_preprocess and make_gradcam_heatmap ---\n",
        "# (These functions were previously defined in other cells/files as part of the overall solution outline)\n",
        "def load_and_preprocess(path):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    img = tf.convert_to_tensor(np.array(img), dtype=tf.float32) / 255.0\n",
        "    img = tf.image.resize(img, [224, 224])\n",
        "    return img\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        loss = predictions[:, 0]  # sigmoid output\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + 1e-8)\n",
        "    heatmap = tf.image.resize(heatmap[..., tf.newaxis], (img_array.shape[1], img_array.shape[2])).numpy()\n",
        "    return heatmap.squeeze()\n",
        "\n",
        "\n",
        "MODEL_PATH = \"saved_model/my_cxr_model\"\n",
        "\n",
        "try:\n",
        "    MODEL = tf.keras.models.load_model(MODEL_PATH)\n",
        "except Exception as e:\n",
        "    st.warning(f\"Could not load model from {MODEL_PATH}: {e}. Creating a dummy model.\")\n",
        "    dummy_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    dummy_model.predict(np.zeros((1, 224, 224, 3)))\n",
        "    MODEL = dummy_model\n",
        "\n",
        "st.title(\"CXR Pneumonia Classifier — Demo (research use only)\")\n",
        "st.markdown(\"**Warning:** This tool is investigational. Not for clinical use. See README for validation details.\")\n",
        "\n",
        "uploaded = st.file_uploader(\"Upload chest X-ray (PNG/JPEG/DICOM)\", type=['png','jpg','jpeg','dcm'])\n",
        "if uploaded:\n",
        "    try:\n",
        "        img_pil = Image.open(uploaded).convert('RGB')\n",
        "        st.image(img_pil, caption='Uploaded image', use_column_width=True)\n",
        "\n",
        "        import tempfile\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as temp_file:\n",
        "            img_pil.save(temp_file.name)\n",
        "            temp_path = temp_file.name\n",
        "\n",
        "        inp = load_and_preprocess(temp_path)\n",
        "        inp_exp = tf.expand_dims(inp, axis=0)\n",
        "        pred = MODEL.predict(inp_exp)[0][0] # sigmoid -> probability of pneumonia\n",
        "        st.metric(\"Pneumonia probability\", f\"{pred:.3f}\")\n",
        "\n",
        "        if st.checkbox(\"Show Grad-CAM\"):\n",
        "            last_conv_layer_name = 'top_conv' if 'EfficientNetB0' in str(MODEL) else MODEL.layers[-3].name\n",
        "            heatmap = make_gradcam_heatmap(inp_exp, MODEL, last_conv_layer_name=last_conv_layer_name)\n",
        "\n",
        "            import matplotlib.pyplot as plt\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.imshow(img_pil.resize((224,224)))\n",
        "            ax.imshow(heatmap, alpha=0.4, cmap='jet')\n",
        "            ax.axis('off')\n",
        "            st.pyplot(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Could not process image: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_C2HpIjRzT7",
        "outputId": "445a07b1-8ec6-4b29-9a69-62094fb4bbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-25 18:55:44.681 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-25 18:55:44.918 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.921 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.923 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.923 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.924 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.926 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.927 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.927 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.928 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-25 18:55:44.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9049b31c"
      },
      "source": [
        "# Task\n",
        "The user has approved the plan. I will now proceed with the following steps:\n",
        "\n",
        "1.  **Create Dummy Dataset**: I will create the necessary directory structure (`dataset/train/class_a`, `dataset/train/class_b`, `dataset/validation/class_a`, `dataset/validation/class_b`) and generate a few placeholder image files within them. This will allow `ImageDataGenerator.flow_from_directory` to find the specified paths and initialize correctly.\n",
        "2.  **Modify and Execute Cell**: I will modify cell `ww8UokW6OwjQ` to include the code for creating the dummy dataset structure and placeholder images at the beginning of the cell. Then, I will execute the modified cell to run the `ImageDataGenerator` setup and model definition.\n",
        "3.  **Final Task**: I will confirm that the `ImageDataGenerator` is now able to create generators from the dummy directories, and that the CNN model is built and compiled without further file-related errors.\n",
        "\n",
        "I will begin by creating the dummy dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b26423e"
      },
      "source": [
        "## Create Dummy Dataset\n",
        "\n",
        "### Subtask:\n",
        "Create the necessary directory structure (`dataset/train/class_a`, `dataset/train/class_b`, `dataset/validation/class_a`, `dataset/validation/class_b`) and generate a few placeholder image files within them. This will allow `ImageDataGenerator.flow_from_directory` to find the specified paths and initialize correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9bc25a1"
      },
      "source": [
        "**Reasoning**:\n",
        "To create the dummy dataset structure and placeholder images, I need to use the `os` module for directory operations and `PIL.Image` for image creation. The code will set up the necessary folders and populate them with a few sample images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f40c9a4",
        "outputId": "41737a14-86c1-4361-8946-ae9266908322"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Define the base path for the dataset\n",
        "dataset_base_path = 'dataset'\n",
        "\n",
        "# 2. Define the subdirectories to create\n",
        "subdirectories = [\n",
        "    'train/class_a',\n",
        "    'train/class_b',\n",
        "    'validation/class_a',\n",
        "    'validation/class_b'\n",
        "]\n",
        "\n",
        "# 3. Create the directory structure and placeholder images\n",
        "for subdir in subdirectories:\n",
        "    path = os.path.join(dataset_base_path, subdir)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Created directory: {path}\")\n",
        "\n",
        "    # Generate a few dummy image files for each class subdirectory\n",
        "    if 'class_a' in subdir:\n",
        "        color = 'red'\n",
        "    elif 'class_b' in subdir:\n",
        "        color = 'blue'\n",
        "    else:\n",
        "        color = 'green'\n",
        "\n",
        "    for i in range(3): # Create 3 images per class\n",
        "        dummy_image = Image.new('RGB', (100, 100), color=color)\n",
        "        image_filename = os.path.join(path, f'image_{i+1}.png')\n",
        "        dummy_image.save(image_filename)\n",
        "        print(f\"  Created placeholder image: {image_filename}\")\n",
        "\n",
        "print(\"Dummy dataset structure and images created successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: dataset/train/class_a\n",
            "  Created placeholder image: dataset/train/class_a/image_1.png\n",
            "  Created placeholder image: dataset/train/class_a/image_2.png\n",
            "  Created placeholder image: dataset/train/class_a/image_3.png\n",
            "Created directory: dataset/train/class_b\n",
            "  Created placeholder image: dataset/train/class_b/image_1.png\n",
            "  Created placeholder image: dataset/train/class_b/image_2.png\n",
            "  Created placeholder image: dataset/train/class_b/image_3.png\n",
            "Created directory: dataset/validation/class_a\n",
            "  Created placeholder image: dataset/validation/class_a/image_1.png\n",
            "  Created placeholder image: dataset/validation/class_a/image_2.png\n",
            "  Created placeholder image: dataset/validation/class_a/image_3.png\n",
            "Created directory: dataset/validation/class_b\n",
            "  Created placeholder image: dataset/validation/class_b/image_1.png\n",
            "  Created placeholder image: dataset/validation/class_b/image_2.png\n",
            "  Created placeholder image: dataset/validation/class_b/image_3.png\n",
            "Dummy dataset structure and images created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ffd7bd2"
      },
      "source": [
        "**Reasoning**:\n",
        "The dummy dataset has been successfully created in the previous step. Now I will execute the original cell `ww8UokW6OwjQ` to confirm that `ImageDataGenerator` can now create generators from the dummy directories and that the CNN model can be built and compiled without file-related errors. There is no need to modify the cell, as the dummy data already exists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "402cb3f6",
        "outputId": "18e9efcf-0d67-4e00-dda8-f2c20b21df65"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ----------------------------------\n",
        "# 1. ImageDataGenerator for Preprocessing\n",
        "# ----------------------------------\n",
        "\n",
        "train_path = \"dataset/train\"\n",
        "val_path = \"dataset/validation\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,         # normalize images\n",
        "    rotation_range=20,       # data augmentation\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# Create Generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(150, 150),   # resize all images\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # for multi-class classification\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_path,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# ----------------------------------\n",
        "# 2. Build CNN Model\n",
        "# ----------------------------------\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ----------------------------------\n",
        "# 3. Compile Model\n",
        "# ----------------------------------\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ----------------------------------\n",
        "# 4. Train the Model\n",
        "# ----------------------------------\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# ----------------------------------\n",
        "# 5. Evaluate the Model\n",
        "# ----------------------------------\n",
        "\n",
        "loss, acc = model.evaluate(val_generator)\n",
        "print(\"Validation Accuracy:\", acc)\n",
        "print(\"Validation Loss:\", loss)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 images belonging to 2 classes.\n",
            "Found 6 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_17\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_17\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_26 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_27 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_28 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m9,470,208\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m514\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,470,208</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,563,970\u001b[0m (36.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,563,970</span> (36.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,563,970\u001b[0m (36.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,563,970</span> (36.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.5000 - loss: 0.6896 - val_accuracy: 1.0000 - val_loss: 0.0797\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0740 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 572ms/step - accuracy: 1.0000 - loss: 0.0258 - val_accuracy: 1.0000 - val_loss: 2.9802e-06\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 1.0000 - loss: 8.3446e-07 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Validation Accuracy: 1.0\n",
            "Validation Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b472cf67"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code execution successfully demonstrated that the `ImageDataGenerator` can now find the dummy dataset directories and create generators, and the CNN model can be built, compiled, and trained without the `FileNotFoundError`. This completes the task as planned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2380eb58",
        "outputId": "acbe1817-5b47-4c14-a796-7e33975bf168"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Define the base path for the dataset\n",
        "dataset_base_path = 'dataset'\n",
        "\n",
        "# 2. Define the subdirectories to create\n",
        "subdirectories = [\n",
        "    'train/class_a',\n",
        "    'train/class_b',\n",
        "    'validation/class_a',\n",
        "    'validation/class_b'\n",
        "]\n",
        "\n",
        "# 3. Create the directory structure and placeholder images\n",
        "for subdir in subdirectories:\n",
        "    path = os.path.join(dataset_base_path, subdir)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Created directory: {path}\")\n",
        "\n",
        "    # Generate a few dummy image files for each class subdirectory\n",
        "    if 'class_a' in subdir:\n",
        "        color = 'red'\n",
        "    elif 'class_b' in subdir:\n",
        "        color = 'blue'\n",
        "    else:\n",
        "        color = 'green'\n",
        "\n",
        "    for i in range(3): # Create 3 images per class\n",
        "        dummy_image = Image.new('RGB', (100, 100), color=color)\n",
        "        image_filename = os.path.join(path, f'image_{i+1}.png')\n",
        "        dummy_image.save(image_filename)\n",
        "        print(f\"  Created placeholder image: {image_filename}\")\n",
        "\n",
        "print(\"Dummy dataset structure and images created successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: dataset/train/class_a\n",
            "  Created placeholder image: dataset/train/class_a/image_1.png\n",
            "  Created placeholder image: dataset/train/class_a/image_2.png\n",
            "  Created placeholder image: dataset/train/class_a/image_3.png\n",
            "Created directory: dataset/train/class_b\n",
            "  Created placeholder image: dataset/train/class_b/image_1.png\n",
            "  Created placeholder image: dataset/train/class_b/image_2.png\n",
            "  Created placeholder image: dataset/train/class_b/image_3.png\n",
            "Created directory: dataset/validation/class_a\n",
            "  Created placeholder image: dataset/validation/class_a/image_1.png\n",
            "  Created placeholder image: dataset/validation/class_a/image_2.png\n",
            "  Created placeholder image: dataset/validation/class_a/image_3.png\n",
            "Created directory: dataset/validation/class_b\n",
            "  Created placeholder image: dataset/validation/class_b/image_1.png\n",
            "  Created placeholder image: dataset/validation/class_b/image_2.png\n",
            "  Created placeholder image: dataset/validation/class_b/image_3.png\n",
            "Dummy dataset structure and images created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ee1e60b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The dummy dataset creation was successful, establishing the required directory structure (`dataset/train/class_a`, `dataset/train/class_b`, `dataset/validation/class_a`, `dataset/validation/class_b`) and generating 3 placeholder images for each class subdirectory.\n",
        "*   The `ImageDataGenerator.flow_from_directory` successfully identified \"6 images belonging to 2 classes\" across both training and validation sets, indicating that the initial `FileNotFoundError` has been resolved.\n",
        "*   The CNN model was successfully defined, compiled, and trained for 10 epochs without encountering any file-related issues.\n",
        "*   Model evaluation on the validation set resulted in a \"Validation Accuracy: 1.0\" and \"Validation Loss: 0.0,\" demonstrating the model's ability to learn the simple dummy data.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The successful setup with dummy data confirms the robustness of the data loading and model training pipeline. The `ImageDataGenerator` and CNN model architecture are correctly implemented to handle the expected directory structure.\n",
        "*   The next critical step is to replace the placeholder dummy images with actual, diverse image data relevant to the problem domain to obtain meaningful training results and evaluate real-world performance.\n"
      ]
    }
  ]
}