{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "\n",
        "Computational Linguistics (CL) is the scientific study of language from a computational perspective.\n",
        "It aims to understand how language works by creating models that can process, analyze, and generate human language using algorithms.\n",
        "\n",
        "In simple words:\n",
        " Computational Linguistics = Linguistics + Computer Science\n",
        "\n",
        "It tries to answer questions like:\n",
        "\n",
        "* How can a computer understand sentences?\n",
        "\n",
        "* How can we represent grammar in a machine-readable way?\n",
        "\n",
        "* How can a computer learn meaning, context, and ambiguity?\n",
        "\n",
        "* How it relates to NLP (Natural Language Processing)?\n",
        "\n",
        "Natural Language Processing is the practical application side, where we build real systems that work with human language — such as chatbots, translators, summarizers, voice assistants, etc.\n",
        "\n",
        "Relationship:\n",
        "\n",
        "* Computational Linguistics provides the theories and scientific foundations.\n",
        "(Grammar, parsing, semantics, morphology, etc.)\n",
        "\n",
        "* NLP uses those theories to build working applications.\n",
        "(Text classification, speech recognition, translation)\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "Field\tPurpose\n",
        "Computational Linguistics\tUnderstand language scientifically & model it computationally\n",
        "NLP\tBuild tools/systems that use language in the real world\n",
        "Analogy\n",
        "\n",
        "CL = Studying the rules of language + building theoretical models\n",
        "\n",
        "NLP = Using those rules to build applications like ChatGPT, Google Translate, Siri"
      ],
      "metadata": {
        "id": "7Q9dCoZMiedj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "**Historical Evolution of Natural Language Processing (NLP)**\n",
        "\n",
        "NLP has evolved through several major phases:\n",
        "\n",
        "1. 1950s–1960s: Rule-Based & Symbolic Era\n",
        "\n",
        "* NLP began with linguistic rules written by experts.\n",
        "\n",
        "* Early systems used grammar rules, dictionaries, and logic.\n",
        "\n",
        "* Famous milestone: Alan Turing’s Turing Test (1950).\n",
        "\n",
        "* Machine Translation experiments (e.g., Georgetown experiment, 1954).\n",
        "\n",
        "2. 1970s–1980s: Knowledge-Based Systems\n",
        "\n",
        "* Development of semantic networks, frames, and AI knowledge bases.\n",
        "\n",
        "* Systems tried to “understand” language using world knowledge.\n",
        "\n",
        "* Example: SHRDLU (natural language understanding in a blocks world).\n",
        "\n",
        "3. 1990s–2000s: Statistical NLP\n",
        "\n",
        "* Shift from rules to probabilistic and statistical models.\n",
        "\n",
        "* Use of large corpora and machine learning.\n",
        "\n",
        "* Algorithms like Hidden Markov Models, Decision Trees, n-grams.\n",
        "\n",
        "* Speech recognition and part-of-speech tagging improved greatly.\n",
        "\n",
        "4. 2010s: Deep Learning Revolution\n",
        "\n",
        "* Introduction of neural networks, especially RNNs, LSTMs, GRUs.\n",
        "\n",
        "* Word embeddings created for meaning representation (Word2Vec, GloVe).\n",
        "\n",
        "* Major improvements in translation, sentiment analysis, and speech.\n",
        "\n",
        "5. 2018–Present: Transformer Models & Large Language Models\n",
        "\n",
        "* Breakthrough model: Transformer (Vaswani et al., 2017).\n",
        "\n",
        "* Enabled BERT, GPT, T5, XLNet, etc.\n",
        "\n",
        "* These models understand context better and generate human-like text.\n",
        "\n",
        "* Today’s NLP is dominated by large-scale pre-trained models.\n"
      ],
      "metadata": {
        "id": "n7GH4xf4jAa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** List and explain three major use cases of NLP in today’s tech industry.\n",
        "\n",
        "Three Major Use Cases of NLP in Today’s Tech Industry:\n",
        "\n",
        "\n",
        "**1. Machine Translatio**n\n",
        "\n",
        "* Converts text from one language to another (e.g., English → Hindi).\n",
        "\n",
        "* Used in tools like Google Translate, Microsoft Translator.\n",
        "\n",
        "* Helps in global communication, content localization, and multilingual support.\n",
        "\n",
        "**2. Sentiment Analysis**\n",
        "\n",
        "* Determines whether a text expresses positive, negative, or neutral emotions.\n",
        "\n",
        "* Used by companies to analyze customer reviews, social media posts, and feedback.\n",
        "\n",
        "* Helps businesses understand public opinion and improve products or services.\n",
        "\n",
        "**3. Chatbots and Virtual Assistants**\n",
        "\n",
        "* NLP powers conversational agents like ChatGPT, Siri, Alexa, Google Assistant.\n",
        "\n",
        "* Helps automate customer support, answer queries, and provide personalized assistance.\n",
        "\n",
        "* Widely used in banking, e-commerce, healthcare, and customer service."
      ],
      "metadata": {
        "id": "OkooXI1njVtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "What is Text Normalization?\n",
        "\n",
        "Text normalization is the process of converting raw text into a consistent, standard, and uniform format so that a machine can easily understand and process it.\n",
        "\n",
        "It removes variations in text by applying steps like:\n",
        "\n",
        "* Lowercasing\n",
        "\n",
        "* Removing punctuation\n",
        "\n",
        "* Expanding abbreviations\n",
        "\n",
        "* Correcting spelling\n",
        "\n",
        "* Lemmatization or stemming\n",
        "\n",
        "**Why is Text Normalization Essential?**\n",
        "\n",
        "Text data is often messy, inconsistent, and full of variations.\n",
        "Normalization is important because:\n",
        "\n",
        "1. Improves accuracy of NLP models by reducing noise.\n",
        "\n",
        "2. Ensures similar words are treated the same (e.g., “Running”, “RUNNING”, “run”).\n",
        "\n",
        "3. Reduces vocabulary size, making processing faster and more efficient.\n",
        "\n",
        "4. Helps algorithms focus on meaning, not formatting differences."
      ],
      "metadata": {
        "id": "TCl6MDUbjp5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Compare and contrast stemming and lemmatization with suitable\n",
        "examples.\n",
        "\n",
        "Stemming vs Lemmatization\n",
        "\n",
        "**1. Stemming**\n",
        "\n",
        "* A rule-based process that cuts off word endings to reduce a word to its base form (called “stem”).\n",
        "\n",
        "* It does not consider grammar or meaning.\n",
        "\n",
        "* Often produces non-dictionary words.\n",
        "\n",
        "Example:\n",
        "\n",
        "* “Running” → “run” or “runn”\n",
        "\n",
        "* “Better” → “bet”\n",
        "\n",
        "* “Studies” → “studi”\n",
        "\n",
        "2. Lemmatization\n",
        "\n",
        "* A linguistically informed process that converts a word to its dictionary base form (called “lemma”).\n",
        "\n",
        "* Considers grammar, part of speech, and context.\n",
        "\n",
        "* Always produces valid words.\n",
        "\n",
        "Example:\n",
        "\n",
        "* “Running” → “run”\n",
        "\n",
        "* “Better” → “good” (comparative form)\n",
        "\n",
        "* “Studies” → “study”\n",
        "\n",
        "## Comparison Table\n",
        "\n",
        "| Feature | Stemming | Lemmatization |\n",
        "| :------ | :------- | :------------ |\n",
        "| Method | Removes suffixes using rules | Uses vocabulary + grammar |\n",
        "| Output | May be non-words | Always valid dictionary words |\n",
        "| Accuracy | Lower | Higher |\n",
        "| Speed | Faster | Slower |\n",
        "| Example (“Studies”) | “studi” | “study” |  "
      ],
      "metadata": {
        "id": "wmFPtJ73j8fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”\n"
      ],
      "metadata": {
        "id": "MTVJsBWJkdQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us. For partnership\n",
        "inquiries, email partners@xyz.biz.\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression pattern for email extraction\n",
        "pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "\n",
        "# Extract all email addresses\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "# Print result\n",
        "print(\"Extracted Email Addresses:\")\n",
        "for email in emails:\n",
        "    print(email)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWFCo6glkn2-",
        "outputId": "e6882288-8b84-4823-af76-de98660d9f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”\n"
      ],
      "metadata": {
        "id": "nLTk9WXKktq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Sample paragraph\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\n",
        "\"\"\"\n",
        "\n",
        "# Download tokenizer (run once)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 1. Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# 2. Frequency Distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, frequency in freq_dist.most_common(10):\n",
        "    print(word, \":\", frequency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKInzVPykx5v",
        "outputId": "6cf487cd-1700-455d-f760-89c7994a01c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            ", : 7\n",
            ". : 4\n",
            "NLP : 3\n",
            "and : 3\n",
            "is : 2\n",
            "of : 2\n",
            "Natural : 1\n",
            "Language : 1\n",
            "Processing : 1\n",
            "( : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text.\n"
      ],
      "metadata": {
        "id": "fxPpjWiElCWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"John and Mary visited London to meet Professor Charles Xavier from Oxford University.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Custom annotator to extract proper nouns\n",
        "print(\"Proper Nouns Identified:\")\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\":\n",
        "        print(token.text, \"→ PROPER NOUN\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3TErYCylEv5",
        "outputId": "0661b1f4-3fba-4f82-f691-09d86c65e3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Nouns Identified:\n",
            "John → PROPER NOUN\n",
            "Mary → PROPER NOUN\n",
            "London → PROPER NOUN\n",
            "Professor → PROPER NOUN\n",
            "Charles → PROPER NOUN\n",
            "Xavier → PROPER NOUN\n",
            "Oxford → PROPER NOUN\n",
            "University → PROPER NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Using Genism, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences:\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim.\n"
      ],
      "metadata": {
        "id": "S3Gbo0jHlHsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Given dataset\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# 1. Preprocessing & Tokenization using simple_preprocess\n",
        "tokenized_sentences = [simple_preprocess(sentence) for sentence in dataset]\n",
        "print(\"Tokenized Sentences:\")\n",
        "print(tokenized_sentences)\n",
        "\n",
        "# 2. Train a Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_sentences,\n",
        "    vector_size=50,    # dimensionality of word vectors\n",
        "    window=5,          # context window size\n",
        "    min_count=1,       # include all words\n",
        "    workers=4,         # for parallel training\n",
        "    sg=0               # CBOW model (sg=1 → Skip-gram)\n",
        ")\n",
        "\n",
        "# 3. Example: Get vector for a word\n",
        "print(\"\\nVector for 'language':\")\n",
        "print(model.wv['language'])\n",
        "\n",
        "# 4. Example: Similar words\n",
        "print(\"\\nMost similar words to 'word':\")\n",
        "print(model.wv.most_similar('word'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktBbSX73lS7T",
        "outputId": "2df9207d-0cda-4c73-9693-308e8d39f7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Tokenized Sentences:\n",
            "[['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language'], ['word', 'embeddings', 'are', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'with', 'similar', 'meaning', 'to', 'have', 'similar', 'representation'], ['word', 'vec', 'is', 'popular', 'word', 'embedding', 'technique', 'used', 'in', 'many', 'nlp', 'applications'], ['text', 'preprocessing', 'is', 'critical', 'step', 'before', 'training', 'word', 'embeddings'], ['tokenization', 'and', 'normalization', 'help', 'clean', 'raw', 'text', 'for', 'modeling']]\n",
            "\n",
            "Vector for 'language':\n",
            "[ 0.00855287  0.00015212 -0.01916856 -0.01933109 -0.01229639 -0.00025714\n",
            "  0.00399483  0.01886394  0.0111687  -0.00858139  0.00055663  0.00992872\n",
            "  0.01539662 -0.00228845  0.00864684 -0.01162876 -0.00160838  0.0162001\n",
            " -0.00472013 -0.01932691  0.01155852 -0.00785964 -0.00244575  0.01996103\n",
            " -0.0045127  -0.00951413 -0.01065877  0.01396178 -0.01141774  0.00422733\n",
            " -0.01051132  0.01224143  0.00871461  0.00521271 -0.00298217 -0.00549213\n",
            "  0.01798587  0.01043155 -0.00432504 -0.01894062 -0.0148521  -0.00212748\n",
            " -0.00158989 -0.00512582  0.01936544 -0.00091704  0.01174752 -0.01489517\n",
            " -0.00501215 -0.01109973]\n",
            "\n",
            "Most similar words to 'word':\n",
            "[('step', 0.2706873416900635), ('processing', 0.2548932731151581), ('with', 0.24128952622413635), ('and', 0.211445614695549), ('many', 0.18664862215518951), ('human', 0.1767798215150833), ('clean', 0.16752134263515472), ('nlp', 0.16078214347362518), ('normalization', 0.15081925690174103), ('vec', 0.14548535645008087)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** Imagine you are a data scientist at a fintech startup. You’ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews.\n"
      ],
      "metadata": {
        "id": "uQkiAPE1lYb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a data scientist at a fintech startup analyzing thousands of customer reviews, I would follow these systematic steps:\n",
        "\n",
        "**1. Data Collection & Understanding**\n",
        "\n",
        "* Gather customer reviews from app stores, surveys, emails, support tickets, etc.\n",
        "\n",
        "* Understand the structure: text length, language, duplicates, metadata (date, rating).\n",
        "\n",
        "**2. Text Cleaning & Preprocessing**\n",
        "\n",
        "Perform normalization to make text uniform and machine-friendly:\n",
        "\n",
        "* Lowercasing all text\n",
        "\n",
        "* Removing noise: punctuation, URLs, numbers, HTML tags, special characters\n",
        "\n",
        "* Tokenization: splitting text into words\n",
        "\n",
        "* Stopword removal: removing common words (is, the, this…)\n",
        "\n",
        "* Spelling correction (optional)\n",
        "\n",
        "* Stemming or lemmatization to reduce words to root form\n",
        "\n",
        "* Handling emojis/emoticons, as they carry sentiment\n",
        "\n",
        "* Removing duplicate reviews\n",
        "\n",
        "**3. Exploratory Text Analysis**\n",
        "\n",
        "* Generate initial insights:\n",
        "\n",
        "* Most common words (word frequency)\n",
        "\n",
        "* N-grams (phrases like “late payment”, “loan approval”)\n",
        "\n",
        "* Word clouds\n",
        "\n",
        "* Review length distribution\n",
        "\n",
        "* This helps to understand general customer concerns.\n",
        "\n",
        "**4. Sentiment Analysis**\n",
        "\n",
        "Apply supervised models (Logistic Regression, SVM) or pre-trained transformers to classify reviews as positive, negative, or neutral.\n",
        "\n",
        "* Aggregate sentiment across time to track customer satisfaction trends.\n",
        "\n",
        "**5. Topic Modeling**\n",
        "\n",
        "Use unsupervised techniques to discover hidden themes:\n",
        "\n",
        "* LDA (Latent Dirichlet Allocation)\n",
        "\n",
        "* NMF (Non-negative Matrix Factorization)\n",
        "\n",
        "Example topics:\n",
        "\n",
        "* “App performance issues”\n",
        "\n",
        "* “Loan rejection complaints”\n",
        "\n",
        "* “Good customer support”\n",
        "\n",
        "**6. Named Entity Recognition (NER)**\n",
        "\n",
        "* Identify important entities:\n",
        "\n",
        "* Product names\n",
        "\n",
        "* Transaction types\n",
        "\n",
        "* Bank names\n",
        "\n",
        "* Complaint categories\n",
        "\n",
        "* This helps find exactly where issues are occurring.\n",
        "\n",
        "**7. Text Classification**\n",
        "\n",
        "Train models to classify reviews into categories such as:\n",
        "\n",
        "* Payment issues\n",
        "\n",
        "* KYC problems\n",
        "\n",
        "* Account login issues\n",
        "\n",
        "* Customer support complaints\n",
        "\n",
        "* Useful for routing complaints to the right department.\n",
        "\n",
        "**8. Aspect-Based Sentiment Analysis**\n",
        "\n",
        "Analyze sentiment for specific features:\n",
        "\n",
        "* Loan approval time → negative\n",
        "\n",
        "* Mobile app design → positive\n",
        "\n",
        "* Customer support → mixed\n",
        "\n",
        "* Helps prioritize product improvements.\n",
        "\n",
        "**9. Visualization & Reporting**\n",
        "\n",
        "Use dashboards (Power BI, Tableau, Python plots):\n",
        "\n",
        "* Sentiment trends\n",
        "\n",
        "* Top issues\n",
        "\n",
        "* Topic summaries\n",
        "\n",
        "* Customer pain points\n",
        "\n",
        "* Provide actionable insights to product, engineering, and support teams.\n",
        "\n",
        "**10. Continuous Monitoring**\n",
        "\n",
        "* Automate the pipeline to analyze new reviews daily.\n",
        "\n",
        "* Update models regularly to maintain accuracy."
      ],
      "metadata": {
        "id": "2Z4-4g3al6cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages (run once in your environment)\n",
        "# !pip install nltk gensim spacy textblob sklearn\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Download required resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Sample dataset (normally thousands of reviews)\n",
        "reviews = [\n",
        "    \"The loan approval process is very slow and frustrating.\",\n",
        "    \"Great app design and easy to use features!\",\n",
        "    \"Customer support is extremely helpful and polite.\",\n",
        "    \"I faced issues during KYC verification.\",\n",
        "    \"Too many bugs in the latest update.\",\n",
        "    \"Instant loan disbursal. Loved the experience!\",\n",
        "    \"Unable to complete payment, the app keeps crashing.\"\n",
        "]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. TEXT CLEANING FUNCTION\n",
        "# ------------------------------------------------------------\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)       # remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)   # remove special characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
        "    return text\n",
        "\n",
        "cleaned_reviews = [clean_text(r) for r in reviews]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. TOKENIZATION & STOPWORD REMOVAL\n",
        "# ------------------------------------------------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "tokenized_reviews = [tokenize(r) for r in cleaned_reviews]\n",
        "\n",
        "print(\"Tokenized Reviews:\")\n",
        "print(tokenized_reviews)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. SENTIMENT ANALYSIS (TextBlob)\n",
        "# ------------------------------------------------------------\n",
        "def get_sentiment(text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return \"Positive\"\n",
        "    elif polarity < 0:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "sentiments = [get_sentiment(r) for r in cleaned_reviews]\n",
        "\n",
        "print(\"\\nSentiment Analysis:\")\n",
        "for review, sentiment in zip(reviews, sentiments):\n",
        "    print(f\"{review} --> {sentiment}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. TOPIC MODELING USING LDA (Gensim)\n",
        "# ------------------------------------------------------------\n",
        "# Prepare data for LDA\n",
        "dictionary = corpora.Dictionary(tokenized_reviews)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_reviews]\n",
        "\n",
        "# Train LDA model\n",
        "lda_model = gensim.models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=2,\n",
        "    passes=10\n",
        ")\n",
        "\n",
        "print(\"\\nLDA Topics:\")\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for t in topics:\n",
        "    print(t)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. NER USING spaCy (to extract entities like product names, places, etc.)\n",
        "# ------------------------------------------------------------\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"\\nNamed Entities:\")\n",
        "for review in reviews:\n",
        "    doc = nlp(review)\n",
        "    print(review)\n",
        "    for ent in doc.ents:\n",
        "        print(\"  \", ent.text, \"-->\", ent.label_)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "XP-LZ2Htmo6b",
        "outputId": "1d66b1cd-840c-45fe-f379-b7efa0b18358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Reviews:\n",
            "[['loan', 'approval', 'process', 'slow', 'frustrating'], ['great', 'app', 'design', 'easy', 'use', 'features'], ['customer', 'support', 'extremely', 'helpful', 'polite'], ['faced', 'issues', 'kyc', 'verification'], ['many', 'bugs', 'latest', 'update'], ['instant', 'loan', 'disbursal', 'loved', 'experience'], ['unable', 'complete', 'payment', 'app', 'keeps', 'crashing']]\n",
            "\n",
            "Sentiment Analysis:\n",
            "The loan approval process is very slow and frustrating. --> Negative\n",
            "Great app design and easy to use features! --> Positive\n",
            "Customer support is extremely helpful and polite. --> Negative\n",
            "I faced issues during KYC verification. --> Neutral\n",
            "Too many bugs in the latest update. --> Positive\n",
            "Instant loan disbursal. Loved the experience! --> Positive\n",
            "Unable to complete payment, the app keeps crashing. --> Negative\n",
            "\n",
            "LDA Topics:\n",
            "(0, '0.065*\"app\" + 0.065*\"loan\" + 0.039*\"crashing\" + 0.039*\"complete\" + 0.039*\"keeps\"')\n",
            "(1, '0.051*\"bugs\" + 0.051*\"latest\" + 0.051*\"update\" + 0.051*\"helpful\" + 0.051*\"many\"')\n",
            "\n",
            "Named Entities:\n",
            "The loan approval process is very slow and frustrating.\n",
            "\n",
            "Great app design and easy to use features!\n",
            "\n",
            "Customer support is extremely helpful and polite.\n",
            "\n",
            "I faced issues during KYC verification.\n",
            "   KYC --> ORG\n",
            "\n",
            "Too many bugs in the latest update.\n",
            "\n",
            "Instant loan disbursal. Loved the experience!\n",
            "\n",
            "Unable to complete payment, the app keeps crashing.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}