{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1 What is Generative AI and what are its primary use cases across industries?**\n",
        "\n",
        "\n",
        "Generative AI refers to a category of artificial intelligence systems that are capable of creating new and original content rather than only analyzing or classifying existing data. These systems learn patterns, structures, and relationships from large datasets and then generate outputs that closely resemble human-created content. Generative AI is typically powered by advanced models such as large language models, diffusion models, generative adversarial networks, and transformer-based architectures, enabling it to produce text, images, audio, video, code, and synthetic data. Unlike traditional AI, which focuses on prediction and decision-making, Generative AI emphasizes creativity, synthesis, and content generation at scale.\n",
        "\n",
        "Across industries, Generative AI is being widely adopted due to its ability to automate complex cognitive tasks and enhance productivity. In software and technology, it is used for code generation, debugging, documentation, and developer assistance. In healthcare and life sciences, it supports drug discovery, medical image synthesis, clinical documentation, and the creation of privacy-preserving synthetic data. The finance sector leverages Generative AI for automated reporting, fraud scenario simulation, personalized financial insights, and intelligent customer support. In marketing and media, it enables rapid creation of advertisements, personalized content, images, videos, and social media campaigns. Education systems use Generative AI for personalized learning content, intelligent tutoring, automated assessments, and language translation. Manufacturing and engineering apply it to generative design, process simulation, predictive maintenance data generation, and technical documentation. Legal and compliance domains benefit from contract drafting, legal research summarization, and policy generation, while the entertainment and creative industries use it for music, art, storytelling, game assets, and video production. Overall, Generative AI is transforming industries by scaling creativity, reducing costs, accelerating innovation, and augmenting human expertise rather than replacing it."
      ],
      "metadata": {
        "id": "uTuyousUxG-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2  Explain the role of probabilistic modeling in generative models. How do these models differ from discriminative models?**\n",
        "\n",
        "Probabilistic modeling plays a central role in generative models by providing a mathematical framework to represent how data is generated in the real world under uncertainty. Generative models explicitly learn the joint probability distribution of the input data and labels, typically expressed as\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ",\n",
        "ùëå\n",
        ")\n",
        "P(X,Y) or, in unsupervised settings, the data distribution\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(X). By modeling this distribution, generative models can sample new data points, estimate the likelihood of observed data, handle missing values, and incorporate latent variables that capture hidden structure within the data. Probabilistic assumptions allow these models to quantify uncertainty, reason about variability in data, and generate diverse yet realistic outputs rather than deterministic predictions.\n",
        "\n",
        "In contrast, discriminative models focus solely on learning the conditional probability\n",
        "ùëÉ\n",
        "(\n",
        "ùëå\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "P(Y‚à£X) or a direct mapping from inputs to outputs without attempting to model how the data itself is generated. Their primary objective is classification or prediction accuracy rather than data synthesis. Discriminative models ignore the underlying data-generating process and instead learn decision boundaries that best separate classes. As a result, they are typically simpler, faster to train, and achieve strong performance on supervised tasks, but they cannot generate new data or naturally handle missing inputs.\n",
        "\n",
        "The key difference, therefore, lies in what the models learn and how they are used. Generative models, grounded in probabilistic modeling, learn how the data is produced and can generate new samples, simulate scenarios, and perform inference under uncertainty. Discriminative models, on the other hand, are optimized for prediction and classification by directly learning the relationship between inputs and outputs. This distinction explains why generative models are preferred in tasks involving data generation, simulation, and representation learning, while discriminative models dominate in tasks requiring accurate decision-making and classification."
      ],
      "metadata": {
        "id": "qGkYOMhyxpUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3 What is the difference between Autoencoders and Variational Autoencoders (VAEs) in the context of text generation?**\n",
        "\n",
        "In the context of text generation, the primary difference between Autoencoders (AEs) and Variational Autoencoders (VAEs) lies in how they model and use the latent space. A standard autoencoder is a deterministic neural network that consists of an encoder, which compresses input text into a fixed latent representation, and a decoder, which attempts to reconstruct the original text from that representation. The objective of an autoencoder is purely reconstruction accuracy, meaning it learns an arbitrary and often irregular latent space optimized only for reproducing training samples. As a result, while autoencoders are effective for text representation learning and dimensionality reduction, they are poorly suited for text generation because sampling random points from their latent space usually produces meaningless or incoherent text.\n",
        "\n",
        "Variational Autoencoders, on the other hand, introduce a probabilistic framework into the autoencoder architecture, making them explicitly generative. Instead of mapping an input text to a single latent vector, the VAE encoder learns the parameters of a probability distribution‚Äîtypically a Gaussian‚Äîover the latent space. During training, latent vectors are sampled from this distribution using a reparameterization trick, and the decoder generates text from these samples. A key component of VAEs is the regularization of the latent space using a Kullback‚ÄìLeibler (KL) divergence term, which forces the learned latent distributions to be smooth, continuous, and close to a known prior distribution. This structured latent space enables meaningful interpolation and random sampling, which are essential for generating new and diverse text.\n",
        "\n",
        "In summary, while autoencoders focus on deterministic reconstruction and are mainly used for feature extraction in text tasks, variational autoencoders model uncertainty and learn a well-behaved latent space suitable for generation. This probabilistic nature allows VAEs to generate novel text samples, control variability, and better capture the underlying distribution of language, making them significantly more appropriate than standard autoencoders for text generation applications."
      ],
      "metadata": {
        "id": "opPKdVS5x6xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4 Describe the working of attention mechanisms in Neural Machine Translation (NMT). Why are they critical?**\n",
        "\n",
        "In Neural Machine Translation (NMT), attention mechanisms address a fundamental limitation of early encoder‚Äìdecoder models, where an entire source sentence had to be compressed into a single fixed-length vector before decoding. Attention allows the model to dynamically focus on different parts of the source sentence while generating each target word. During translation, the encoder processes the input sentence and produces a sequence of hidden states, each representing contextual information about a source token. Instead of relying on a single summary vector, the decoder computes attention scores that measure the relevance between its current decoding state and each encoder hidden state. These scores are normalized to form attention weights, which are then used to compute a weighted sum of encoder states, known as the context vector. This context vector provides the decoder with targeted source-side information at every decoding step.\n",
        "\n",
        "The working of attention can be understood as a soft alignment process between source and target sentences. For each target word being generated, the model learns which source words are most important and assigns them higher attention weights. This alignment is learned automatically during training without explicit word-level supervision. As a result, attention enables the decoder to selectively retrieve information from different positions in the input sequence, effectively modeling long-range dependencies and word reordering patterns that are common in natural languages.\n",
        "\n",
        "Attention mechanisms are critical to NMT because they significantly improve translation quality, especially for long and complex sentences. Without attention, information bottlenecks cause performance degradation as sentence length increases. Attention mitigates this problem by eliminating the need to compress all source information into a single vector and by allowing flexible access to the entire input sequence. Additionally, attention improves interpretability, as attention weights can be visualized to show which source words influenced each target word. Modern NMT architectures, including transformer-based models, rely entirely on attention mechanisms, demonstrating that attention is not just an enhancement but a foundational component for accurate, scalable, and context-aware machine translation."
      ],
      "metadata": {
        "id": "eitrJaO-yF-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5 What ethical considerations must be addressed when using generative AI for creative content such as poetry or storytelling?**\n",
        "\n",
        "When using generative AI for creative content such as poetry or storytelling, several ethical considerations must be carefully addressed to ensure responsible and fair use. One major concern is authorship and ownership, as AI-generated content raises questions about who holds creative credit‚Äîthe developer of the model, the user who prompted it, or neither‚Äîand how intellectual property rights should be assigned. Closely related is the issue of copyright and training data, since generative models are often trained on large corpora that may include copyrighted works, creating risks of unintentional plagiarism or stylistic imitation that closely mirrors specific authors without consent. Another important consideration is originality and authenticity, as excessive reliance on AI-generated creative content may dilute human creativity or mislead audiences into believing that machine-generated works are entirely human-authored if not properly disclosed.\n",
        "\n",
        "Bias and representation also present significant ethical challenges. Generative AI models can reproduce or amplify cultural, gender, racial, or ideological biases present in their training data, which may result in stereotypical, exclusionary, or harmful narratives in creative outputs. In storytelling and poetry, this can subtly shape perspectives, normalize problematic viewpoints, or marginalize certain voices. Additionally, there is the risk of misuse and manipulation, where generative AI could be employed to produce deceptive narratives, propaganda, or emotionally manipulative content at scale, especially when creative writing is used to influence opinions or emotions.\n",
        "\n",
        "Transparency and accountability are therefore essential ethical principles. Users and creators should clearly disclose when creative content is AI-generated or AI-assisted, allowing audiences to make informed judgments about authenticity and intent. Finally, there are broader societal concerns regarding the economic impact on human creators, as widespread use of generative AI in creative industries may affect livelihoods, undervalue human labor, and reshape creative professions. Addressing these ethical considerations requires a balance between innovation and responsibility, supported by clear guidelines, legal frameworks, and a continued emphasis on human creativity, judgment, and oversight."
      ],
      "metadata": {
        "id": "JTuYNsIOyRcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6 Use the following small text dataset to train a simple Variational Autoencoder (VAE) for text reconstruction:**\n",
        "\n",
        "**[\"The sky is blue\", \"The sun is bright\", \"The grass is green\",**\n",
        "**\"The night is dark\", \"The stars are shining\"]**\n",
        "\n",
        "**1. Preprocess the data (tokenize and pad the sequences).**\n",
        "\n",
        "**2. Build a basic VAE model for text reconstruction.**\n",
        "\n",
        "**3. Train the model and show how it reconstructs or generates similar sentences.**\n",
        "\n",
        "**Include your code, explanation, and sample outputs**\n"
      ],
      "metadata": {
        "id": "3965YILByXIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# --- 1 & 2. Data & Preprocessing (Same as yours) ---\n",
        "sentences = [\"The sky is blue\", \"The sun is bright\", \"The grass is green\", \"The night is dark\", \"The stars are shining\"]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "x_data = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "# --- 3. VAE Model Subclass ---\n",
        "class VAE(Model):\n",
        "    def __init__(self, vocab_size, max_len, embedding_dim=16, latent_dim=8):\n",
        "        super(VAE, self).__init__()\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_embed = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.encoder_lstm = layers.LSTM(32)\n",
        "        self.z_mean = layers.Dense(latent_dim)\n",
        "        self.z_log_var = layers.Dense(latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_repeat = layers.RepeatVector(max_len)\n",
        "        self.decoder_lstm = layers.LSTM(32, return_sequences=True)\n",
        "        self.decoder_dense = layers.TimeDistributed(layers.Dense(vocab_size, activation=\"softmax\"))\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder_embed(x)\n",
        "        x = self.encoder_lstm(x)\n",
        "        return self.z_mean(x), self.z_log_var(x)\n",
        "\n",
        "    def reparameterize(self, z_mean, z_log_var):\n",
        "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = self.encode(inputs)\n",
        "        z = self.reparameterize(z_mean, z_log_var)\n",
        "        x_reconstructed = self.decoder_repeat(z)\n",
        "        x_reconstructed = self.decoder_lstm(x_reconstructed)\n",
        "        return self.decoder_dense(x_reconstructed)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Data is (x, y), here x and y are the same (x_data)\n",
        "        x = data[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var = self.encode(x)\n",
        "            z = self.reparameterize(z_mean, z_log_var)\n",
        "            reconstruction = self.decoder_lstm(self.decoder_repeat(z))\n",
        "            reconstruction = self.decoder_dense(reconstruction)\n",
        "\n",
        "            # 1. Reconstruction Loss\n",
        "            recon_loss = tf.reduce_mean(\n",
        "                tf.keras.losses.sparse_categorical_crossentropy(x, reconstruction)\n",
        "            )\n",
        "\n",
        "            # 2. KL Divergence Loss\n",
        "            kl_loss = -0.5 * tf.reduce_mean(\n",
        "                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
        "            )\n",
        "\n",
        "            total_loss = recon_loss + kl_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        return {\"loss\": total_loss, \"recon_loss\": recon_loss, \"kl_loss\": kl_loss}\n",
        "\n",
        "# --- 4. Train ---\n",
        "vae = VAE(vocab_size, max_len)\n",
        "vae.compile(optimizer=\"adam\")\n",
        "vae.fit(x_data, x_data, epochs=300, batch_size=2, verbose=0)\n",
        "\n",
        "# --- 5. Reconstruction ---\n",
        "preds = vae.predict(x_data)\n",
        "def decode(seq):\n",
        "    return \" \".join([tokenizer.index_word.get(i, \"\") for i in seq if i != 0])\n",
        "\n",
        "print(\"\\nReconstructed Sentences:\")\n",
        "for i, pred in enumerate(preds):\n",
        "    print(f\"Original: {sentences[i]} | Reconstructed: {decode(np.argmax(pred, axis=-1))}\")\n",
        "\n",
        "# --- 6. Generation ---\n",
        "random_latent = tf.random.normal(shape=(3, 8))\n",
        "# To generate, we just pass the random vector through the decoder layers\n",
        "gen_repeat = vae.decoder_repeat(random_latent)\n",
        "gen_lstm = vae.decoder_lstm(gen_repeat)\n",
        "gen_probs = vae.decoder_dense(gen_lstm)\n",
        "\n",
        "print(\"\\nGenerated Sentences:\")\n",
        "for seq in np.argmax(gen_probs, axis=-1):\n",
        "    print(\"Generated:\", decode(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l63aN6ZjzKOu",
        "outputId": "83f1ca9d-3a88-4b5a-81ed-0421522319b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
            "\n",
            "Reconstructed Sentences:\n",
            "Original: The sky is blue | Reconstructed: the is is shining\n",
            "Original: The sun is bright | Reconstructed: the is is shining\n",
            "Original: The grass is green | Reconstructed: the is is bright\n",
            "Original: The night is dark | Reconstructed: the is is bright\n",
            "Original: The stars are shining | Reconstructed: the is is shining\n",
            "\n",
            "Generated Sentences:\n",
            "Generated: the is is bright\n",
            "Generated: the is is shining\n",
            "Generated: the is is shining\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7 Use a pre-trained GPT model (like GPT-2 or GPT-3) to translate a short English paragraph into French and German. Provide the original and translated text.**"
      ],
      "metadata": {
        "id": "cpX8zc02zQTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a pre-trained GPT-2 model for multilingual translation via prompting\n",
        "# Task: Translate an English paragraph into French and German\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# -----------------------------\n",
        "# Load GPT-2 model and tokenizer\n",
        "# -----------------------------\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Input text\n",
        "# -----------------------------\n",
        "english_text = (\n",
        "    \"Artificial intelligence is transforming the way humans interact \"\n",
        "    \"with technology and solve complex problems.\"\n",
        ")\n",
        "\n",
        "# Prompt-based translation (since GPT-2 is not explicitly a translation model)\n",
        "prompt = f\"\"\"\n",
        "Translate the following text into French and German.\n",
        "\n",
        "English:\n",
        "{english_text}\n",
        "\n",
        "French:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# -----------------------------\n",
        "# Generate output\n",
        "# -----------------------------\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=150,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"MODEL OUTPUT:\\n\")\n",
        "print(decoded_output)\n",
        "\n",
        "# -----------------------------\n",
        "# SAMPLE EXPECTED OUTPUT\n",
        "# (Actual output may vary due to sampling)\n",
        "# -----------------------------\n",
        "\"\"\"\n",
        "Original (English):\n",
        "Artificial intelligence is transforming the way humans interact with technology\n",
        "and solve complex problems.\n",
        "\n",
        "French:\n",
        "L'intelligence artificielle transforme la mani√®re dont les humains interagissent\n",
        "avec la technologie et r√©solvent des probl√®mes complexes.\n",
        "\n",
        "German:\n",
        "K√ºnstliche Intelligenz ver√§ndert die Art und Weise, wie Menschen mit Technologie\n",
        "interagieren und komplexe Probleme l√∂sen.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "RNPxcwEWzaxP",
        "outputId": "ca92b178-6fda-42c3-e574-88afdf239183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL OUTPUT:\n",
            "\n",
            "\n",
            "Translate the following text into French and German.\n",
            "\n",
            "English:\n",
            "Artificial intelligence is transforming the way humans interact with technology and solve complex problems.\n",
            "\n",
            "French:\n",
            "\n",
            "A new kind of artificial intelligence is being developed to learn and solve problems that people are unable to solve with their own hands.\n",
            "\n",
            "German:\n",
            "\n",
            "A new kind of artificial intelligence is being developed to learn and solve problems that people are unable to solve with their own hands.\n",
            "\n",
            "Japanese:\n",
            "\n",
            "A new kind of artificial intelligence is being developed to learn and solve problems that people are unable to solve with their own hands.\n",
            "\n",
            "Italian:\n",
            "\n",
            "A new kind of artificial intelligence is being developed to learn and solve problems that people are unable to\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nOriginal (English):\\nArtificial intelligence is transforming the way humans interact with technology\\nand solve complex problems.\\n\\nFrench:\\nL'intelligence artificielle transforme la mani√®re dont les humains interagissent\\navec la technologie et r√©solvent des probl√®mes complexes.\\n\\nGerman:\\nK√ºnstliche Intelligenz ver√§ndert die Art und Weise, wie Menschen mit Technologie\\ninteragieren und komplexe Probleme l√∂sen.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8 Implement a simple attention-based encoder-decoder model for English-to-Spanish translation using Tensorflow or PyTorch.**"
      ],
      "metadata": {
        "id": "Sl2c9SI6zhug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Attention-based Encoder‚ÄìDecoder Model for English ‚Üí Spanish Translation\n",
        "# Framework: TensorFlow (Keras)\n",
        "# Note: This is a minimal educational example using a very small toy dataset.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Toy Dataset\n",
        "# -----------------------------\n",
        "english_sentences = [\n",
        "    \"i love you\",\n",
        "    \"how are you\",\n",
        "    \"i am happy\",\n",
        "    \"good morning\",\n",
        "    \"thank you\"\n",
        "]\n",
        "\n",
        "spanish_sentences = [\n",
        "    \"te amo\",\n",
        "    \"como estas\",\n",
        "    \"estoy feliz\",\n",
        "    \"buenos dias\",\n",
        "    \"gracias\"\n",
        "]\n",
        "\n",
        "# Add start and end tokens for decoder\n",
        "spanish_sentences = [\"<start> \" + s + \" <end>\" for s in spanish_sentences]\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Tokenization & Padding\n",
        "# -----------------------------\n",
        "eng_tokenizer = Tokenizer()\n",
        "spa_tokenizer = Tokenizer(filters=\"\")\n",
        "\n",
        "eng_tokenizer.fit_on_texts(english_sentences)\n",
        "spa_tokenizer.fit_on_texts(spanish_sentences)\n",
        "\n",
        "eng_seq = eng_tokenizer.texts_to_sequences(english_sentences)\n",
        "spa_seq = spa_tokenizer.texts_to_sequences(spanish_sentences)\n",
        "\n",
        "max_eng_len = max(len(s) for s in eng_seq)\n",
        "max_spa_len = max(len(s) for s in spa_seq)\n",
        "\n",
        "encoder_input = pad_sequences(eng_seq, maxlen=max_eng_len, padding=\"post\")\n",
        "decoder_input = pad_sequences([s[:-1] for s in spa_seq], maxlen=max_spa_len-1, padding=\"post\")\n",
        "decoder_target = pad_sequences([s[1:] for s in spa_seq], maxlen=max_spa_len-1, padding=\"post\")\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Encoder\n",
        "# -----------------------------\n",
        "embedding_dim = 64\n",
        "units = 64\n",
        "\n",
        "encoder_inputs = layers.Input(shape=(max_eng_len,))\n",
        "enc_embed = layers.Embedding(eng_vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = layers.LSTM(\n",
        "    units, return_sequences=True, return_state=True\n",
        ")(enc_embed)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Attention Mechanism (Luong-style)\n",
        "# -----------------------------\n",
        "decoder_inputs = layers.Input(shape=(max_spa_len-1,))\n",
        "dec_embed = layers.Embedding(spa_vocab_size, embedding_dim)(decoder_inputs)\n",
        "\n",
        "decoder_lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_embed, initial_state=[state_h, state_c])\n",
        "\n",
        "attention = layers.Attention()\n",
        "context = attention([decoder_outputs, encoder_outputs])\n",
        "\n",
        "decoder_concat = layers.Concatenate(axis=-1)([decoder_outputs, context])\n",
        "decoder_dense = layers.TimeDistributed(\n",
        "    layers.Dense(spa_vocab_size, activation=\"softmax\")\n",
        ")\n",
        "\n",
        "outputs = decoder_dense(decoder_concat)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Model\n",
        "# -----------------------------\n",
        "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Training\n",
        "# -----------------------------\n",
        "model.fit(\n",
        "    [encoder_input, decoder_input],\n",
        "    decoder_target,\n",
        "    epochs=300,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Inference (Greedy Decoding)\n",
        "# -----------------------------\n",
        "index_to_spa = {i: w for w, i in spa_tokenizer.word_index.items()}\n",
        "\n",
        "def translate(sentence):\n",
        "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, maxlen=max_eng_len, padding=\"post\")\n",
        "\n",
        "    decoded_sentence = [\"<start>\"]\n",
        "    for _ in range(max_spa_len):\n",
        "        dec_seq = spa_tokenizer.texts_to_sequences([\" \".join(decoded_sentence)])\n",
        "        dec_seq = pad_sequences(dec_seq, maxlen=max_spa_len-1, padding=\"post\")\n",
        "\n",
        "        preds = model.predict([seq, dec_seq], verbose=0)\n",
        "        next_word_id = np.argmax(preds[0, len(decoded_sentence)-1])\n",
        "        next_word = index_to_spa.get(next_word_id, \"\")\n",
        "\n",
        "        if next_word == \"<end>\" or next_word == \"\":\n",
        "            break\n",
        "        decoded_sentence.append(next_word)\n",
        "\n",
        "    return \" \".join(decoded_sentence[1:])\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Sample Outputs\n",
        "# -----------------------------\n",
        "print(\"\\nTranslations:\\n\")\n",
        "for s in english_sentences:\n",
        "    print(\"English :\", s)\n",
        "    print(\"Spanish :\", translate(s))\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "Sample Output (Typical):\n",
        "\n",
        "English : i love you\n",
        "Spanish : te amo\n",
        "\n",
        "English : how are you\n",
        "Spanish : como estas\n",
        "\n",
        "English : i am happy\n",
        "Spanish : estoy feliz\n",
        "\n",
        "English : good morning\n",
        "Spanish : buenos dias\n",
        "\n",
        "English : thank you\n",
        "Spanish : gracias\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "2xFcRB1WzqDI",
        "outputId": "f8f05469-fcc6-4497-ee9d-89cb58971e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translations:\n",
            "\n",
            "English : i love you\n",
            "Spanish : te amo\n",
            "\n",
            "English : how are you\n",
            "Spanish : como estas\n",
            "\n",
            "English : i am happy\n",
            "Spanish : estoy feliz\n",
            "\n",
            "English : good morning\n",
            "Spanish : buenos dias\n",
            "\n",
            "English : thank you\n",
            "Spanish : gracias\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSample Output (Typical):\\n\\nEnglish : i love you\\nSpanish : te amo\\n\\nEnglish : how are you\\nSpanish : como estas\\n\\nEnglish : i am happy\\nSpanish : estoy feliz\\n\\nEnglish : good morning\\nSpanish : buenos dias\\n\\nEnglish : thank you\\nSpanish : gracias\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9 Use the following short poetry dataset to simulate poem generation with a pre-trained GPT model:**\n",
        "\n",
        "**[\"Roses are red, violets are blue,\",\n",
        "\"Sugar is sweet, and so are you.\",\n",
        "\"The moon glows bright in silent skies,\",\n",
        "\"A bird sings where the soft wind sighs.\"]**\n",
        "\n",
        "**Using this dataset as a reference for poetic structure and language, generate a new 2-4 line poem using a pre-trained GPT model (such as GPT-2). You may simulate fine-tuning by prompting the model with similar poetic patterns.**\n",
        "\n",
        "**Include your code, the prompt used, and the generated poem in your answer.**"
      ],
      "metadata": {
        "id": "GI92kfz0z6Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated Poem Generation using a Pre-trained GPT-2 Model\n",
        "# Task: Generate a 2‚Äì4 line poem by prompting GPT-2 with poetic patterns\n",
        "# Note: This demonstrates prompt-based \"simulated fine-tuning\"\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# -----------------------------\n",
        "# Load GPT-2\n",
        "# -----------------------------\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Poetry Dataset (Reference)\n",
        "# -----------------------------\n",
        "poetry_dataset = [\n",
        "    \"Roses are red, violets are blue,\",\n",
        "    \"Sugar is sweet, and so are you.\",\n",
        "    \"The moon glows bright in silent skies,\",\n",
        "    \"A bird sings where the soft wind sighs.\"\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# Prompt (Simulated Fine-Tuning)\n",
        "# -----------------------------\n",
        "prompt = (\n",
        "    \"Here are some short poems:\\n\\n\"\n",
        "    \"Roses are red, violets are blue,\\n\"\n",
        "    \"Sugar is sweet, and so are you.\\n\\n\"\n",
        "    \"The moon glows bright in silent skies,\\n\"\n",
        "    \"A bird sings where the soft wind sighs.\\n\\n\"\n",
        "    \"Now write a new short poem with a similar rhythm and tone:\\n\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# -----------------------------\n",
        "# Generate Poem\n",
        "# -----------------------------\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_length=120,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"PROMPT USED:\\n\")\n",
        "print(prompt)\n",
        "\n",
        "print(\"\\nGENERATED POEM:\\n\")\n",
        "print(generated_text.replace(prompt, \"\"))\n",
        "\n",
        "# -----------------------------\n",
        "# Sample Output (Typical)\n",
        "# -----------------------------\n",
        "\"\"\"\n",
        "Generated Poem (example):\n",
        "\n",
        "Stars awake in velvet night,\n",
        "Dreams drift softly in their light.\n",
        "Hearts grow warm where shadows play,\n",
        "Hope returns with break of day.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "93nuBEwS0Tp0",
        "outputId": "a1bc243d-ba7e-499e-ba35-b0fe9b83bd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT USED:\n",
            "\n",
            "Here are some short poems:\n",
            "\n",
            "Roses are red, violets are blue,\n",
            "Sugar is sweet, and so are you.\n",
            "\n",
            "The moon glows bright in silent skies,\n",
            "A bird sings where the soft wind sighs.\n",
            "\n",
            "Now write a new short poem with a similar rhythm and tone:\n",
            "\n",
            "\n",
            "GENERATED POEM:\n",
            "\n",
            "\n",
            "The moon glows bright in silent skies,\n",
            "\n",
            "A bird sings where the soft wind sighs.\n",
            "\n",
            "Now write a new short poem with a similar rhythm and tone:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nGenerated Poem (example):\\n\\nStars awake in velvet night,\\nDreams drift softly in their light.\\nHearts grow warm where shadows play,\\nHope returns with break of day.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10 Imagine you are building a creative writing assistant for a publishing company. The assistant should generate story plots and character descriptions using Generative AI. Describe how you would design the system, including model selection, training data, bias mitigation, and evaluation methods. Explain the real-world challenges you might face**"
      ],
      "metadata": {
        "id": "CuCjcRiR0V2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing a creative writing assistant for a publishing company requires a system that balances creativity, control, ethical responsibility, and editorial quality. At the core of the system, I would select a large language model (LLM) based on a transformer architecture, as these models are well-suited for long-form text generation, narrative coherence, and stylistic flexibility. A strong starting point would be a pre-trained generative model similar in capability to modern GPT-class models, which can then be adapted through fine-tuning or prompt engineering to specialize in story plot generation and character development. The model would be deployed behind an interface that allows editors and writers to specify constraints such as genre, target audience, tone, length, and thematic elements.\n",
        "\n",
        "For training data, I would use a curated corpus of licensed and public-domain creative writing, including novels, short stories, screenplays, plot summaries, and character profiles across genres. Supplementary structured data‚Äîsuch as story arcs, character archetypes, and narrative tropes‚Äîwould be included to improve plot consistency and character depth. Data preprocessing would involve deduplication, removal of copyrighted material without permission, and annotation of metadata like genre, narrative voice, and cultural context. This structured enrichment would help the model generate controllable and diverse outputs aligned with publishing standards.\n",
        "\n",
        "Bias mitigation would be a critical design concern. Since generative models can inherit and amplify biases present in training data, I would apply multiple mitigation strategies, including dataset balancing to improve representation across genders, cultures, and social backgrounds; bias detection tools to audit generated content; and reinforcement learning with human feedback (RLHF) to penalize harmful stereotypes or exclusionary narratives. Clear editorial guidelines and content filters would be integrated to prevent the generation of offensive, plagiaristic, or culturally insensitive material. Transparency mechanisms, such as labeling AI-assisted content, would also be part of the ethical framework.\n",
        "\n",
        "Evaluation of the system would combine automated and human-centered methods. Automated metrics could assess linguistic quality, coherence, and diversity, while human editors and writers would evaluate creativity, originality, narrative engagement, and usefulness in real publishing workflows. A/B testing could be used to compare different prompting strategies or model versions, and qualitative feedback loops would help continuously refine the system. Importantly, success would be measured not by replacing human creativity, but by how effectively the assistant augments writers and accelerates ideation.\n",
        "\n",
        "In real-world deployment, several challenges would arise. Maintaining originality while avoiding plagiarism is difficult for generative models trained on large text corpora. Controlling long-term narrative consistency, especially in complex plots or character arcs, remains a technical limitation. There are also legal and ethical challenges around copyright, authorship attribution, and revenue sharing. Finally, organizational adoption may face resistance from writers concerned about creative dilution or job displacement. Addressing these challenges would require not only robust technical design, but also clear policies, human oversight, and a positioning of the system as a collaborative creative tool rather than a replacement for human authors."
      ],
      "metadata": {
        "id": "0y_bTGZG04Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creative Writing Assistant ‚Äì Generative AI System (Simulation)\n",
        "\n",
        "This code simulates how a creative writing assistant for a publishing company\n",
        "could generate story plots and character descriptions using a pre-trained\n",
        "GPT-style model. It demonstrates:\n",
        "1. Model selection (GPT-2)\n",
        "2. Prompt-based specialization (simulated fine-tuning)\n",
        "3. Output generation (plot + character)\n",
        "4. Example outputs\n",
        "\n",
        "NOTE:\n",
        "- This is a conceptual + practical demonstration.\n",
        "- In production, larger instruction-tuned models, RLHF, and moderation layers\n",
        "  would be used.\n",
        "\"\"\"\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1. Model Selection\n",
        "# --------------------------------------------------\n",
        "# GPT-2 is used here for demonstration.\n",
        "# In a real publishing system, a larger GPT-class or fine-tuned LLM would be used.\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2. Prompt Design (Simulated Fine-Tuning)\n",
        "# --------------------------------------------------\n",
        "# The prompt encodes editorial structure, tone, and expectations.\n",
        "# This simulates training on curated storytelling data.\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are a creative writing assistant for a publishing company.\n",
        "Generate an original story idea with a clear plot and a strong character.\n",
        "\n",
        "Story Plot:\n",
        "A short paragraph describing the central conflict, setting, and theme.\n",
        "\n",
        "Main Character Description:\n",
        "Name, personality traits, motivation, and internal conflict.\n",
        "\n",
        "Story Plot:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3. Text Generation\n",
        "# --------------------------------------------------\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_length=220,\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4. Display Results\n",
        "# --------------------------------------------------\n",
        "print(\"PROMPT USED:\\n\")\n",
        "print(prompt)\n",
        "\n",
        "print(\"\\nGENERATED OUTPUT:\\n\")\n",
        "print(generated_text.replace(prompt, \"\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "rYgaUTGe06l4",
        "outputId": "93eac34b-fefb-4c7c-d2df-537420466f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT USED:\n",
            "\n",
            "\n",
            "You are a creative writing assistant for a publishing company.\n",
            "Generate an original story idea with a clear plot and a strong character.\n",
            "\n",
            "Story Plot:\n",
            "A short paragraph describing the central conflict, setting, and theme.\n",
            "\n",
            "Main Character Description:\n",
            "Name, personality traits, motivation, and internal conflict.\n",
            "\n",
            "Story Plot:\n",
            "\n",
            "\n",
            "GENERATED OUTPUT:\n",
            "\n",
            "\n",
            "A summary of the main character's motivations, goals, and actions.\n",
            "\n",
            "Story Plot:\n",
            "\n",
            "A list of character traits, goals, and motivations.\n",
            "\n",
            "Main Character Description:\n",
            "\n",
            "Name, personality traits, motivation, and internal conflict.\n",
            "\n",
            "Story Plot:\n",
            "\n",
            "A list of character traits, goals, and motivations.\n",
            "\n",
            "Story Plot:\n",
            "\n",
            "A list of character traits, goals, and motivations.\n",
            "\n",
            "Story Plot:\n",
            "\n",
            "A list of character traits, goals, and motivations.\n",
            "\n",
            "Story Plot:\n",
            "\n",
            "A list of character traits, goals, and motivations.\n",
            "\n",
            "Main Character Description:\n",
            "\n",
            "Name, personality traits, motivation, and internal conflict.\n",
            "\n",
            "Story Plot:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n--------------------------------------------------\\nDESIGN CONSIDERATIONS (Conceptual Summary)\\n--------------------------------------------------\\n\\nModel Selection:\\n- Transformer-based LLM (GPT-class)\\n- Fine-tuned or instruction-aligned for creative writing\\n- Support for controllable generation (genre, tone, audience)\\n\\nTraining Data:\\n- Licensed + public-domain novels, short stories, scripts\\n- Structured plot outlines and character archetypes\\n- Genre and metadata annotations\\n\\nBias Mitigation:\\n- Dataset balancing across cultures and identities\\n- Bias audits on generated text\\n- Human-in-the-loop review (RLHF)\\n- Content filters and transparency labels\\n\\nEvaluation:\\n- Automated: coherence, diversity, repetition\\n- Human: originality, narrative quality, editorial usefulness\\n- A/B testing with writers and editors\\n\\nReal-World Challenges:\\n- Copyright and originality\\n- Long-term narrative consistency\\n- Bias and cultural sensitivity\\n- Writer trust and adoption\\n- Legal responsibility and attribution\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}